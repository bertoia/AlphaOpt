\def\year{2017}\relax

\documentclass[letterpaper]{article}
\usepackage{aaai17}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Gaussian Process for Hyper-Parameter Optimization of Machine Learning Algorithms)
/Author (Put All Your Authors Here, Separated by Commas)}
\setcounter{secnumdepth}{1}
 \begin{document}
% The file aaai.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%
\title{Gaussian Process for Hyper-Parameter Optimization of Machine Learning Algorithms}
\author{
National University of Singapore \\
CS4246 Group 7 \AND
\normalsize\normalfont\textbf{Cheong, Xuan Hao Filbert A0121261Y} \\
\normalsize\normalfont\textbf{Gay, Ming Jian Davis A0111035A} \\
\normalsize\normalfont\textbf{Karen Ang Mei Yi A0116603R} \And
\normalsize\normalfont\textbf{Quek, Chang Rong Sebastian A0121520A} \\
\normalsize\normalfont\textbf{Vincent Seng Boon Chin A0121501E} \\
\normalsize\normalfont\textbf{Xu, Ruofan A0100965J}
}

\maketitle
\begin{abstract}
Hyper-parameter tuning has traditionally been done by humans who are efficient in getting reasonable performance within a few trials. Popular algorithms such as grid search or randomized search can be expensive. In this paper, we explore the use of Gaussian Processes to maximize the performance with minimum cost through Bayesian optimization
\end{abstract}

\section{Introduction}
\noindent Machine learning algorithms usually involves careful tuning of hyper-parameters, regularization terms and optimization parameters. Performance of the algorithms are usually sensitive to the parameters involved. The parameters are often continuous in nature, making parameter tuning a difficult optimization problem for machine learning practitioners.

One common strategy is grid search. It is a sequential search of cartesian product of individual parameter domain space with specified step intervals. A more efficient method is a manual subset of grid search, where one manually change the parameters with changing arbitrary step sizes depending on the accuracy of the trained machine learning model to achieve a satisfactory maximum accuracy of the train models.

A stochastic method, called random search, can also be employed where parameters are randomly picked within a predefined space.


\section{Motivating Application}
Machine learning algorithms is widely used in academia and industry in learning problems such as classification and regression. The challenge of hyper-parameter optimization in large and multilayer models is a direct impediment to scientific progress.


In some machine learning algorithms such as neural network, there are no definite and explicit method to select optimal parameters (number of hidden layers, number of units for each layer, etc).


The tuning algorithms outlined above are expensive as they search through parameter space exhaustively. This problem is usually mitigated by increasing search step or narrowing parameter space. For example in tuning of Support Vector Machine (SVM) with RBF kernel, it is recommended that C and γ are searched exponentially in base 2 . A more economical approach could be to search in base 10.


It would be ideal to have an algorithm that cleverly move through the parameter space in every trial to get a set of parameters that gives us maximum accuracy with as little trials as possible. It is with this motivation that we proposed Bayesian optimization based on Gaussian Process predictive model.

\section{Technical Approach}
A Gaussian process is a collection of random variables, any finite subset of which have a multivariate Gaussian distribution. It is completely specified by a mean function $\mu(\textbf{x})$ and the covariance function $k(\textbf{x}, \textbf{x}')$. For a real process $f(\textbf{x})$:

\begin{align*}
	\mu(\textbf{x}) &= E[f(\textbf{x})] \\
	k(\textbf{x}, \textbf{x}') &= E[(f(\textbf{x}) - \mu(\textbf{x}))(f(\textbf{x}') - \mu(\textbf{x}')] \\
\end{align*}

The GP can then be denoted as:
\[f(\textbf{x}) \sim \mathcal{GP}(\mu(\textbf{x}), k(\textbf{x}, \textbf{x}'))\]

We assume that our data $\mathcal{D} = \{(x_1, y_1), \ldots, (x_i, y_i)\}$ are such that $y_i$ are noisy observations originating from a GP-distributed random function $f(\textbf{x}_i)$ such that:

\begin{align*}
	y_i &= f(\textbf{x}_i) + \epsilon_i \\
	\epsilon_i &\sim \mathcal{N}(0, \sigma_n^2)
\end{align*}

Given $\textbf{y} = [y_1y_2\ldots y_i]^\top$, suppose we have a new input $\textbf{x}_*$ for which we would like to obtain a prediction for. In other words, we would like to infer $p(f_* \mid \textbf{y})$. Then, the predictive mean and variance from the GP can be given by:

\begin{align*}
	&E[f_* \mid \textbf{y}] = \mu(\textbf{x}_*) + \textbf{k}_*^\top (\textbf{K} + \sigma_n^2 \textbf{I})^{-1} (\textbf{y} - \boldsymbol{\mu}) \\
	&V[f_* \mid y] = k(\textbf{x}_*, \textbf{x}_*) - \textbf{k}_*^\top (\textbf{K} + \sigma_n^2 \textbf{I})^{-1} \textbf{k}_* \\
\end{align*}

\subsection{Problem Definition}
An optimization problem typically involves the finding of a minimum f(x) by exploiting a model to pick the next point to evaluate the function. As training a machine learning algorithm can be expensive, it is worthwhile to invest more computational cost in picking a promising set of hyperparameters to evaluate next. In this paper, we choose a Gaussian Process model as our prior and an acquisition function to pick the next point to evaluate by constructing a utility function from the model posterior. 

Besides the number of trials, we also take consideration of the time for each trial and pick the point that not only yield good result but fast to evaluate. 
\subsection{Model Definition}
In this paper, we assume that the performance measures for hyperparameters follow a Gaussian distribution. Given n set of hyperparameters which have been evaluated, we predict the performance measure of the next set.

In the cost-sensitive model, we maintains another GP model to predict the execution time given the same input.


\subsection{Acquisition Functions}

\subsection{Covariance Functions}
The GP function is mainly characterized by its covariance function after normalizing the data to attain a mean of 0. The covariance function produces a covariance matrix which is utilised by the Gaussian process model for inference.
We propose the use of the ARD Mat´ern 5/2 kernel
\begin{align*}
	k (\textbf{x},\textbf{x}') = \sigma^2(1+\sqrt5r+\frac{5}{3}r^2)\exp(-\sqrt5r)
\end{align*}

The Mat'ern 5/2 kernel is commonly used in the areas of machine learning and multivariate statistical analysis on metric spaces. Due to its stationary and isotropic nature, it is able to produce similar predictive output for two inputs that are d units distant from each other. 5/2 is chosen for its ability to produce smooth and yet sensitive curves.

\subsection{Cost-sensitive Modeling}
While it is important to find a good set of hyperparameters in the fewest number of trials, we should keep in mind that the execution time for different regions of the parameter spaces can vary significantly. In the situation of limited budget, such as time or money, we propose a cost-sensitive model to select the next point based on expected improvement per second. A separate GP model is used to model the evaluation time of each setting of hyperparameters. By dividing the expected improvement of each point by its cost, the new acquisition function selects points that balance between good performance and cheap evaluation cost.

\subsection{Learning the hyper-parameters}

\section{Experimental Setup}

\subsection{Procedure}

\section{Experimental Evaluation}

\section{Improvements}


\section{References}


\section{Roles and Contributions}

\begin{description}
\item [Cheong Xuan Hao Filbert]
\item [Gay Ming Jian Davis]
\item [Karen Ang Mei Yi]
\item [Quek Chang Rong Sebastian]
\item [Vincent Seng Boon Chin]
\item [Xu Ruofan]
\end{description}

\end{document}
