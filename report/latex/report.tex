\def\year{2017}\relax

\documentclass[letterpaper]{article}
\usepackage{aaai17}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Bayesian Optimisation of Machine Learning Algorithms)
/Author ()}
\setcounter{secnumdepth}{1}
\DeclareMathOperator*{\argmax}{arg\,max}
 \begin{document}
% The file aaai.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%
\title{Bayesian Optimisation of Machine Learning Algorithms}
\author{
National University of Singapore \\
CS4246 Group 7 \AND
\normalsize\normalfont\textbf{Cheong, Xuan Hao Filbert A0121261Y} \\
\normalsize\normalfont\textbf{Gay, Ming Jian Davis A0111035A} \\
\normalsize\normalfont\textbf{Karen Ang Mei Yi A0116603R} \And
\normalsize\normalfont\textbf{Quek, Chang Rong Sebastian A0121520A} \\
\normalsize\normalfont\textbf{Vincent Seng Boon Chin A0121501E} \\
\normalsize\normalfont\textbf{Xu, Ruofan A0100965J}
}

\maketitle
\begin{abstract}
Machine learning algorithms have grown in popularity over the past few years.
This increased interest and usage also implies an increasing need for
hyperparameter tuning. Traditionally done by human experts, the process is
nonetheless a black art in itself. Experts often use heuristics to guide their
tuning efforts. This is not invulnerable to cognitive biases. Moreover, humans
are unlikely to find a result close to the optimum, especially if there are
numerous hyperparameters and when hyperparameters are correlated in some way
not easily inferred. In this paper, we explore the use of Gaussian Processes to
automate this problem through the use of Bayesian Optimisation.
\end{abstract}

\section{Introduction}
\noindent Machine learning algorithms often involves careful tuning of hyperparameters.
As an example, the hyperparameters of a neural network include the learning rate,
batch size and number of hidden units. Given the black-box nature of this algorithm,
there is no definite and explicit method to select the optimal hyperparameters.
Not only is the performance of such algorithms particularly sensitive to the
parameters involved, this process of hyperparameter optimisation is traditionally
done by a human, who is unlikely to find the optimum easily. The parameters are
also often continuous in nature, making this a difficult optimization problem for
machine learning practitioners.

Currently, the two common strategies employed for tuning are grid search and
random search. The former involves the uniform division of the entire search
space. This is easily seen with a simple example of two parameters, $\theta_1$ and
$\theta_2$, which are allowed to vary along the continuous domain of $(0, 1)$.
Suppose we divided the search space for each parameter at a step interval of 0.5,
that is, the set of values: $\Theta=\{0.0, 0.5, 1.0\}$. Then, the points that one
would evaluate the algorithm at would be given by the cartesian product: $\Theta \times \Theta$.
The second strategy simply involves the random selection of a number of points
within the search space. Notably, not only is this strategy easy to implement, it
often performs comparably better than grid search $[3]$.

\section{Motivating Application}
Machine learning algorithms are widely used in academia and industry in learning
problems such as classification and regression. Automating hyperparameter optimisation
can better improve the performance of these algorithms, which can indirectly lead
to greater scientific or industrial advances and innovations.

The current tuning strategies outlined above have their limitations. Grid search essentially
conducts an exhaustive search in the parameter space. Computationally, this is expensive
when the parameter space is continuous and has a large domain size. This problem
is usually mitigated by increasing the search step interval
or by narrowing the parameter space. For example, in the tuning of Support Vector Machines
(SVM) with the radial basis function (RBF) kernel, it is recommended that $\textit{C}$ and $\gamma$ are searched
exponentially in base 2: $\textit{C}=\{2^{-5},2^{-3},...,2^{15}\}$ and
$\gamma=\{2^{-15},2^{-13},...,2^{2}\}$. A more economical approach could be to search
$\textit{C}$ exponentially in base 10 for a coarser search
($\textit{C}=\{10^{-5},10^{-3},...,10^{15}\}$) and searching over a smaller domain
exponentially in base 2 for $\gamma$ ($\gamma=\{2^{-6},2^{-4},...,2^{2}\}$). On the other hand, while random
search is more effective than grid search especially in high-dimensionsal spaces,
it is stochastic in nature and termination conditions (maximum iterations or threshold change)
have to be specified. Therefore the trade-off between model performance and time needed
for tuning is arbitrarily determined.

However, random search performs poorly when tuning algorithms with numerous hyperparameters for planning and scheduling$[1]$. Consider the case of constructing the optimal finacial asset portfolio using IBM ILOG CPLEX given the investment guidelines. Since the solver has 76 free hyperparameters$[2]$, it is impractical to tune it by hand and using random grid search is inefficient.

In light of the abovementioned problems with conventional strategies, it would be ideal
to have an algorithm that allow us to mathemathically formuate the exploit and explore
trade-off in hyperparameter optimization to train the machine learning model the least
amount of times on parameters that, on each training evaluation instance, acquire the
most information (the concept of "infomation", known as criterion will have to be defined,
as seen later). This can enable the model to reach the near optimum with as little trials as possible.
It is with this motivation that we approach hyperparameter tuning through Bayesian optimisation(BO).

Bayesian optimisation requires two key components one of which is the use of a surrogate model (or response surface)$[5]$ containing our prior belief of the unknown function to approximate the objective.Secondly, for a certain number of iterations, or until
convergence, we pick, according to a heuristic or function, henceforth known as the
acquisition function, the next best point to be evaluated by the objective. The
surrogate model is updated in this process to reflect our new belief. 

In particular, the Gaussian Process(GP) model is used as the surrogate model
for the objective function, which in this case, is the machine learning algorithm.
The Gaussian Process model, being a distribution over functions, is especially apt
to model this unknown, non-convex, and complex objective function. The
initial Gaussian Process model prescribes a prior over the objective, which is
subsequently updated with new data that is observed.
Thus, producing a full predictive distribution at each search point instead of a single prediction.
 
Therefore GP is suitable as a prior for BO compared to other models such as bayesian neural networks$[6]$ is due to the following reasons: (i)We can obtain the predictive distribution of the model easily, (ii) a probablistic treatment of the hyperparameters allows us to discover general properties of the surface such as the smoothness from past samples and (iii) provides a feasible criterion to obtain the next sample from the model easily. Lastly, since evaluating the objective function is expensive, GP model can work well with having few samples unlike other powerful models which require large number of samples.  

This differentiates the strategy of Bayesian optimisation from other tuning
strategies mentioned previously. Instead of discarding information about new points
that we have queried the objective for, Bayesian optimisation cleverly exploits this
knowledge by updating the prior. As a result, this can provide improvements in time
efficiency over naive algorithms such as grid search, whereby the computational
cost scales exponentially with the number of parameters.  This is particulary
important in our application where the search space is usually high-dimensional
and extremely large. Since the Gaussian Process model provides predictive uncertainty
in the posterior, we can exploit this in the acquisition function, which essentially
computes the utility of candidate points and is optimised to obtain the next point
to be evaluated.

Inherent in this process is this trade-off between exploration and exploitation.
Clearly, one could simply greedily select the next point with the highest predictive
mean, or a point which gives the highest probability of improvement. However, one
is also likely to fall into local minima or maxima in this case. Without exploration
in areas of high uncertainty, it is not guaranteed that the global optimum can be
found. This trade-off is usually manifest in the acquisition function. In fact,
we will explore various Bayesian optimisation criteria and conduct experiments to
evaluate their performance in our application.

Finally, there are certain Bayesian optimisation algorithms that can satisfy the
requirements of hyperparameter tuning. Firstly, the tuning process is often
expensive. Evaluations of large models dealing with big data can be extremely
costly. Moreover, it is innately useful and beneficial if we can utilise
Bayesian optimisation to improve this search for the optimum. In other contexts,
one may be limited by time or computing resources. Consequently, minimising the
number of function evaluations to obtain a near-optimal result is clearly a concern.
Conversely, one may have access to parallel computing resources. In this aspect,
Bayesian optimisation is flexible in that both sequential and parallel algorithms
are available. In particular, non-myopic and parallel Bayesian optimisation
algorithms can effectively reduce the number of objective function evaluations
as well as the time needed to optimise the objective.

\section{Technical Approach}
% Describe clearly and CONCISELY the technical details of the active learning or Bayesian optimization algorithm that you are using.
As mentioned, Bayesian optimisation is a stochastic approach which attempts to
balance the exploration-exploitation tradeoff in the optimisation of an objective.
In this section, we will describe in greater detail the problem at hand, as well
as explain the models and algorithms used.

\subsection{Problem Definition}
In hyperparameter tuning, we consider the problem of finding the global maximum of
an unknown function $f$:
$$\textbf{x*}=\argmax_{\textbf{x}\in\mathcal{X}}{f(x)}$$
where $\textbf{x} \in \mathbb{R^d}$ and $\mathcal{X}$ is a bounded subset of 
$\mathbb{R}^d$. In general, the process works as follows: the Gaussian Process model
is initialised with a number of observations and acts as a prior.
An acquisition function, $\alpha(\textbf{x})$, then makes use
of the posterior, in particular, the predictive mean and uncertainty,
to compute the utility of a point. A necessary condition is that the computation and
optimisation of this acquisition function is much less costly than the objective.
Optimisation of $\alpha$ gives us the next point to evaluate, $\textbf{x}'$. The
objective function is evaluated at $\textbf{x}'$. Subsequently, the Gaussian Process 
model is updated with the new data point, $(\textbf{x}', y(\textbf{x}'))$, and the
process is repeated until we hit the budget of evaluations allowed, or till
convergence.

\subsection{Model Definition}
In this paper, we assume that the performance measures for hyperparameters follow a 
Gaussian distribution. As mentioned, we model the unknown objective as a Gaussian Process.
Given that we have already evaluated the following set of hyperparameters, 
$\mathcal{D}=\{\textbf{x}_1, \cdots, \textbf{x}_n\}$, the Gaussian 
Process model predicts the performance measure of the next set of hyperparameters,
$\textbf{x}'$.

In the context of hyperparameter tuning, the time taken to evaluate the objective is
also an important consideration. Consequently, we also have a cost model where
another Gaussian Process model is used to predict the execution time for a particular
set of hyperparameters. In this way, we can select points which are not only
predicted to have a high accuracy in the underlying machine learning model, they
are also likely to be fast to evaluate. Our experiments thus employ
a cost-sensitive model that varies in the type of acquisition function used.

\subsection{Cost-sensitive Modeling}
While it is important to find a good set of hyperparameters in the least number of 
trials, we should keep in mind that the execution time for different regions of the 
parameter space can vary significantly. In the situation of limited budget, such as 
time or money, the cost-sensitive model can select the next point based on expected 
improvement per second. In this cost-sensitive model, dividing the expected improvement 
of each point by its cost allows this acquisition function to select points that 
balance between good performance and cheap evaluation cost.

\subsection{Acquisition Functions}
One of the key challenges in Bayesian optimisation is that of balancing the trade-off
between exploration and exploitation. This issue surfaces in the acquisition function,
where one has to select the next point to be explored. A purely exploitative function
will choose points with a high predictive mean. In contrast, a purely exploratory
function will choose points with a high predictive uncertainty.

We gathered a few interesting insights from our research. 
Firstly, the problem of selecting the next point can be viewed as a infinite-armed
bandit problem \cite{hoffman2011portfolio}, which implies some relationship to
reinforcement learning, where, similarly, we receive evaluative and not instructive
feedback. As such, it is interesting to consider the possibility of modelling the
acquisition function with Q-learning in mind. The off-policy nature of the latter is
a truly remarkable quality, which would certainly be valuable in the setting of not
just hyperparameter tuning, but also Bayesian optimisation in general.

Additionally, it is interesting to note that most of the traditional acquisition
functions focus largely on the minimisation of cumulative regret:
$$\sum_{i=1}^{n} f(\textbf{x}*) - f(\textbf{x}')$$
This includes acquisition functions such as GP-UCB. However, more recent 
information-based approaches such as Entropy Search \cite{hennig2012entropy} and 
Predictive Entropy Search approach the problem with an intriguing perspective.
Particularly in the context of hyperparameter tuning, we are often given a budget,
such as the number of function evaluations allowed. This implies some time sensitivity
and a finite horizon for the optimiser. Thus, any regret-bounds proven in the limit
are not especially useful in this context. In fact, one might argue that, if the
budget is the number of function evaluations, it is perfectly alright for the
acquisition function to pick a costly and potentially poor-performing point,
on the condition that this point provides us with the maximal amount of information.
This information gain essentially clears the fog and gives us a better view of the
true objective and hence, the optimum. It is different from the previous approaches
in that only our final posterior and the predicted optimum matters. Evaluations
before the end of our finite time budget need not necessarily be high-performing.

\subsection{Covariance Functions}
The GP function is mainly characterized by its covariance function after normalizing the data to attain a mean of 0. The covariance function produces a covariance matrix which is utilised by the Gaussian process model for inference.
In our paper, we use the Matern 5/2 kernel with Automatic Relevance Determination (ARD).

\begin{align*}
	k (\textbf{x},\textbf{x}') = \sigma^2(1+\sqrt5r+\frac{5}{3}r^2)\exp(-\sqrt5r)
\end{align*}

The Matern 5/2 kernel is commonly used in the areas of machine learning and multivariate statistical analysis on metric spaces. Due to its stationary and isotropic nature, it is able to produce similar predictive output for two inputs that are $d$ units away from each other. $5/2$ is chosen for its ability to produce smooth and yet sensitive curves.

\section{Experimental Setup}

\subsection{Convolutional Neural Network}
he tuning of a multitude of hyperparameters. In our experiments, we tune six hyperparameters of a simple convolutional neural network which has 1 convolution layer, 1 pooling layer and 2 fully connected layers.

\subsubsection{Dataset}
es in the Wild face recognition dataset that has been used by many research institutions, including Google and Facebook. Google reportedly reached an accuracy of 99.63\%.

This dataset contains pictures of famous people collected over the Internet, with each image having one face. We decided to focus on a subset which contains faces of people who have at least 70 images in the dataset. The problem then becomes a 7-class classification task.

\subsubsection{Hyperparameters}
parameters that we are tuning.

\begin{enumerate}

    \item Optimizer learning rate

    \item Dropout rate (prevent overfitting)

    \item Number of epochs

    \item Number of features (for the convolution layer)

    \item Size of features (dimensions of each feature)

    \item Size of pooling layer (dimensions of each filter in the pooling layer)

\end{enumerate}



\subsection{Procedure}

\section{Experimental Evaluation}

\subsection{Support Vector Machine}
SVMs are supervised learning models that construct a hyperplane or set of hyperplanes in high-
or indefinite- dimensional space to divide the labelled data for classification and regression.
The data points which are considered in drawing the separating hyperplane is called a support
vector In reality, not all data are separable by a hyperplane due to noise or outliers, hence in
practice, a soft-margin classification is deployed to allow hyperplanes to be constructed
with a decision margin to make a few mistakes to generalize the data better.

The parameter $\textit{C}$ in SVM is a regularization term which controls overfitting of the soft-margin.
As $\textit{C}$ becomes large, it is more attractive to respect the data points and reduce
the margin. When $\textit{C}$ is small, the SVM will trade misclassification for simplicity
of the decision surface.

Additionally, a kernel can be supplied to map points in data to higher dimension. In our
experiment, we used the RBF kernel.
\begin{align*}
  k (\textbf{x},\textbf{x}') &= \exp(-\gamma||(\textbf{x},\textbf{x}')||)^2 \\
  &\text{where} \ \gamma = \frac{1}{2\sigma^2}
\end{align*}
The parameter $\gamma$ is the inverse of the radius of influence of samples selected by the
model as support vectors. In other words, a smaller $\gamma$ leads to a more constraint
model where the hyperplanes "curves" less.

If $\gamma$ is too large, the radius of the area of influence of the support vectors only
includes the support vector itself and no amount of regularization with $\textit{C}$ will be
able to prevent overfitting.

Therefore the difficulty of tuning SVM with RBF kernel is to balance the value of
$\gamma$ and $\textit{C}$ to generalize the data well. Secondly, since SVM time complexity
is dependent on the number of support vectors, there is a dilemma of choosing a lower value
of $\textit{C}$ will favour models that use less memory and faster to predict at risk of
the model underfitting the data.

\section{Improvements}

\section{References}
% Consider using BibTeX
$[1]$https://arxiv.org/pdf/1206.2944.pdf  
$[2]$https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf  
$[3]$http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf  
$[4]$\text{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.139.9315\&rep=rep1\&type=pdf}  
$[5]$\text{https://arxiv.org/pdf/1012.2599v1.pdf}
$[6]$https://arxiv.org/pdf/1502.05700v2.pdf
 

\section{Roles and Contributions}

\begin{description}
\item [Cheong Xuan Hao Filbert]
\item [Gay Ming Jian Davis]
\item [Karen Ang Mei Yi]
\item [Quek Chang Rong Sebastian]
\item [Vincent Seng Boon Chin]
\item [Xu Ruofan]
\end{description}

\bibliographystyle{aaai}
\bibliography{ref}

\end{document}
